{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# importing libraries"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T09:12:10.017536Z",
     "start_time": "2024-05-03T09:12:04.697660Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# data importing"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T09:12:14.057559Z",
     "start_time": "2024-05-03T09:12:10.018573Z"
    }
   },
   "source": [
    "# Parameters\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_LEN = 250\n",
    "EMBEDDING_DIM = 16\n",
    "MODEL_PATH = 'sentiment_analysis_model.keras'\n",
    "\n",
    "file_path = 'data.csv'\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T09:13:25.709236Z",
     "start_time": "2024-05-03T09:12:14.058567Z"
    }
   },
   "source": [
    "df_shuffled = data.sample(frac=1).reset_index(drop=True)\n",
    "texts = []\n",
    "labels = []\n",
    "for _, row in df_shuffled.iterrows():\n",
    "    texts.append(row.iloc[-1])\n",
    "    label = row.iloc[0]\n",
    "    if label == 0:\n",
    "        labels.append(0)\n",
    "    elif label == 2:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(2)\n",
    "    # labels.append(0 if label == 0 else 1 if label == 2 else 2)\n",
    "\n",
    "texts = np.array(texts)\n",
    "labels = np.array(labels)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# count each word sequence"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:13:31.237831Z",
     "start_time": "2024-04-28T13:12:23.066642Z"
    }
   },
   "source": [
    "# Tokenize and pad the sequences\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=MAX_LEN, value=VOCAB_SIZE-1, padding='post')"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:13:32.232216Z",
     "start_time": "2024-04-28T13:13:31.239846Z"
    }
   },
   "source": [
    "# Save the tokenizer to a file\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:13:32.290872Z",
     "start_time": "2024-04-28T13:13:32.234306Z"
    }
   },
   "source": [
    "# Split data into training and test sets (you might want to do this in a more balanced way)\n",
    "train_data = padded_sequences[:-5000]\n",
    "test_data = padded_sequences[-5000:]\n",
    "train_labels = labels[:-5000]\n",
    "test_labels = labels[-5000:]"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# train model or load existing model"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:13:32.403857Z",
     "start_time": "2024-04-28T13:13:32.293894Z"
    }
   },
   "source": [
    "# Check if saved model exists\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(\"Loading saved model...\")\n",
    "    model = load_model(MODEL_PATH)\n",
    "else:\n",
    "    print(\"Training a new model...\")\n",
    "    # Define the model\n",
    "    model = Sequential([\n",
    "        Embedding(VOCAB_SIZE, EMBEDDING_DIM),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(3, activation='softmax')  # 3 classes: negative, neutral, positive\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(train_data, train_labels, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save(MODEL_PATH)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved model...\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T10:51:43.429703Z",
     "start_time": "2024-04-03T10:51:42.950589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m157/157\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7719 - loss: 0.4549\n",
      "Test accuracy: 77.30%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "loss, accuracy = model.evaluate(test_data, test_labels)\n",
    "print(f\"Test accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T12:07:22.568941Z",
     "start_time": "2024-04-28T12:07:18.219944Z"
    }
   },
   "source": [
    "# Interactive loop for predictions\n",
    "def encode_text(text):\n",
    "    tokens = tf.keras.preprocessing.text.text_to_word_sequence(text)\n",
    "    tokens = [tokenizer.word_index[word] if word in tokenizer.word_index and tokenizer.word_index[word] < VOCAB_SIZE else 0 for word in tokens]\n",
    "    return pad_sequences([tokens], maxlen=MAX_LEN, padding='post', value=VOCAB_SIZE - 1)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Enter a sentence for sentiment analysis (or 'exit' to quit): \")\n",
    "    if user_input.lower() in ['exit', 'e']:\n",
    "        break\n",
    "    \n",
    "    encoded_input = encode_text(user_input)\n",
    "    prediction = np.argmax(model.predict(encoded_input))\n",
    "\n",
    "    if prediction == 0:\n",
    "        print(\"Sentiment: Negative\")\n",
    "    elif prediction == 1:\n",
    "        print(\"Sentiment: Neutral\")\n",
    "    else:\n",
    "        print(\"Sentiment: Positive\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step\n",
      "Sentiment: Positive\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
